
@misc{ballantyneMinskyTheoremSingle2018,
  title = {Minsky's "{{And}} / {{Or}}" {{Theorem}}: {{A Single Perceptron}}'s {{Limitations}}},
  shorttitle = {Minsky's "{{And}} / {{Or}}" {{Theorem}}},
  author = {Ballantyne, Alando},
  year = {2018},
  month = feb,
  journal = {Medium},
  abstract = {In 1969, Marvin Minsky and Seymour Papert published Perceptrons \textemdash{} a historic text that would alter the course of artificial intelligence\ldots},
  howpublished = {https://alan.do/minskys-and-or-theorem-a-single-perceptron-s-limitations-490c63a02e9f},
  langid = {english},
  file = {/Users/lss/Zotero/storage/S974BJ6K/minskys-and-or-theorem-a-single-perceptron-s-limitations-490c63a02e9f.html}
}

@article{copelandAlanTuringAnticipation1996,
  title = {On {{Alan Turing}}'s Anticipation of Connectionism},
  author = {Copeland, B. Jack and Proudfoot, Diane},
  year = {1996},
  month = sep,
  journal = {Synthese},
  volume = {108},
  number = {3},
  pages = {361--377},
  issn = {1573-0964},
  doi = {10.1007/BF00413694},
  abstract = {It is not widely realised that Turing was probably the first person to consider building computing machines out of simple, neuron-like elements connected together into networks in a largely random manner. Turing called his networks `unorganised machines'. By the application of what he described as `appropriate interference, mimicking education' an unorganised machine can be trained to perform any task that a Turing machine can carry out, provided the number of `neurons' is sufficient. Turing proposed simulating both the behaviour of the network and the training process by means of a computer program. We outline Turing's connectionist project of 1948.},
  langid = {english},
  file = {/Users/lss/Zotero/storage/8KHDWTMT/Copeland and Proudfoot - 1996 - On Alan Turing's anticipation of connectionism.pdf}
}

@inproceedings{imperialNaturalLanguageTranslation1994,
  title = {A {{Natural Language Translation Neural Network}}},
  booktitle = {In {{Proceedings}} of the {{International Conference}} on {{New Methods}} in {{Language Processing}} ({{NeMLaP}}},
  author = {Imperial, Nenad Koncar and Koncar, Nenad and Guthrie, Dr Gregory},
  year = {1994},
  pages = {71--77},
  abstract = {proper translation by a user without any expert knowledge of how the computer stores and represents rules. This paper demonstrates the utility of neural networks in precisely this area on a small scale translation problem.  We have tested the ability of neural networks to perform natural language translation. Our results have shown a greatly improved translation accuracy in comparison to the work of R.B. Allen (1987) in translating English into Spanish. A neural network was trained on a set of 10,000 sentences from a total of 24,750 sentences using a novel training algorithm. On a test set of 100 sentences the neural network showed a 98\% sentence accuracy. The neural network had 48 input nodes, 70 nodes in the first hidden layer, 1 node in the third hidden layer, and 36 nodes in the output layer (48-701 -36). A fully connected architecture was used. Connectionist NLP  Research has already shown the usefulness of neural networks in various natural language processing tasks: (Allen, 1987...},
  file = {/Users/lss/Zotero/storage/Y9I2ILFL/Imperial et al. - 1994 - A Natural Language Translation Neural Network.pdf;/Users/lss/Zotero/storage/JGD4GMZK/summary.html}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  langid = {english},
  file = {/Users/lss/Zotero/storage/3ALCKI7X/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{mccullochLogicalCalculusIdeas,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {Mcculloch, Warren S and Pitts, Walter},
  pages = {17},
  langid = {english},
  file = {/Users/lss/Zotero/storage/HRW25EVU/Mcculloch and Pitts - A logical calculus of the ideas immanent in nervou.pdf}
}

@book{minskyPerceptronsIntroductionComputational1969a,
  title = {Perceptrons : An Introduction to Computational Geometry},
  author = {Minsky, Marvin and Papert, Seymour},
  year = {1969},
  publisher = {{The MIT Press}},
  keywords = {Apprentissage automatique,Géométrie -- Informatique,Parallélisme (informatique),Perceptrons},
  note = {Accession Number: bur.76453; Other Notes: 352382102 ouvrage -- Notes bibliogr.: p. 247-253. Index; Publication Type: Book; Physical Description: 258 p ill 23 cm; Language: English}
}

@misc{ProfessorPerceptronPaved,
  title = {Professor's Perceptron Paved the Way for {{AI}} \textendash{} 60 Years Too Soon},
  journal = {Cornell Chronicle},
  abstract = {As the Faculty of Computing and Information Science celebrates its 20th year, Frank Rosenblatt's prescient research into artificial intelligence underscores Cornell's pivotal role in computing~history.},
  howpublished = {https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon},
  langid = {english},
  file = {/Users/lss/Zotero/storage/I8PXRPEE/professors-perceptron-paved-way-ai-60-years-too-soon.html}
}

@article{rosenblattPerceptronProbabilisticModel1958a,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  shorttitle = {The {{Perceptron}}},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  pages = {65--386},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
  file = {/Users/lss/Zotero/storage/ULI349IP/Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf;/Users/lss/Zotero/storage/5PS6PNK9/summary.html}
}

@article{rumelhart1986general,
  title = {A General Framework for Parallel Distributed Processing},
  author = {Rumelhart, David E and Hinton, Geoffrey E and McClelland, James L and others},
  year = {1986},
  journal = {Parallel distributed processing: Explorations in the microstructure of cognition},
  volume = {1},
  number = {45-76},
  pages = {26},
  publisher = {{Cambridge, MA: MIT Press}},
  file = {/Users/lss/Zotero/storage/C75HS9PK/Chap2_PDP86.pdf}
}


